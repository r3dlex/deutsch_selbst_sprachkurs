# Dataflow Analysis # 

Die Idee heute, ist dieses Thema abzuschließen. 
Wir haben es zuvor zwei Mal darüber gesprochen aber nicht mit jedem in einer Besprechung.

## Slide 2 ##

Diese ersten Folien befassen sich nur mit den Verantwortlichkeiten des Teams und dem Aufbau und der kontinuierlichen Weiterentwicklung der C++ Code Regeln.

Ich kann die überspringen, oder?

## Slide 4 ##

### Annahmen (bevor ich die Analyse gemacht habe) ###

 * Vollständiges Build ausführen
 * Falschpositive müssen analysiert werden.
 * Ich sollte eine Grundlage für den Vergleich finden.


### Einschränkungen ###

 * Ich nutze das OpenCV als Beispiel Projekt
 * Ich analysiere nur das OpenCV Core Modul
 * Die Analyse Zeit musst probiert werden.


### Die Methodik ###

  * Erstellen einer Coverity Warnings Baseline (Coverity-Basislinie)
  * und auch einer Helix Warnings Basline (Helix-Grundlinie)
  * Scannen von OpenCV mit beiden Tools (core modules)
  * Extrahieren einzelner Werkzeuginformationen
  * Leistungen vergleichen


## Slide 5 ##

Bei Helix lenkt die Anzahl der Null Pointers die Aufmerksamkeit auf sich. Sind die Falschpositive? Und die anderen Findings?

Auf den nächsten Folien zeige ich einige Datei Proben, die entnommen wurden. 

Zehn verschiedene Dateien mit Befunden wurden gesampelt, um die Leistung für verschiedenen Warnings des Helix zu finden. 
j
Dasselbe wurde für Coverity durchgeführt.

### Optional Teil ###

Bitte beachtet ihr, da es sich hierbei nicht um einen formalen Vergleich handelte, sondern nur um eine Zufallsstichprobe, sind die Dateien selbst nicht so wichtig, sondern nur die Menge der gesampelten Probleme.

Wenn ich eine umfassende Analyse durchführen würde, wäre es das Ehrlichste, jeden Fehler in jeder Datei zu analysieren, um eine genaue Zahl zu ermitteln. 
Dieser Prozess würde definitiv einen enormen Aufwand und viel Zeit erfordern.


## Slide 6 ##

Da man bei Probenahmen nicht wirklich 100% falschpositive Ergebnisse erzielen kann. 

Hier sage ich einfach, sie liegen bei 99%.

99% Null pointers sind falschpositive. 


## Slide 7 ##

Das gleich beim Null Pointer Dereference und uninitialisierten Pointer.


## Slide 8, 9, 10 ##

Helix hat nicht so viele Kontrollflussanalysen wie Coverity, aber es hat ein schlimmes echtes positives Verhältnis.

Das ist anders bei der Arithmetic Fehler und Unreachable Code Warnings. Aber sie sind, die einfachsten Fehler mit einem Tool zu finden.
Hier ist Floating Point Fehler eine Außnahme. Die waren beim Helix deaktiviert.


## Slide 11 ##

Nach der Deaktivierung der Warnings (Warnungen) sind die Ergebnisse wirklich interessanter.

Von der ursprünglichen Anzahl der Warnings (Warnungen) bleiben nach der Deaktivierung nur 7% (übrig = remain).


## Slide 12 ##

### Das Helix Résumé ###

* Mit aktivierten Pointers Warnings gibt es keine Chance, Helix zu einsetzen.
* Die Analyse dauert 8 Stunden.
* Viele Dateien waren herausgefiltert.



## Slide 13 ##

Da ich Helix schon analysiert habe, ich stelle mir die Fragen: 
1. Was sind die gemeinsamen Warnings?
2. Was sind die unterschiedlichen Ergebnisse?

Von einem ersten Blick sind die Werkzeuge völlig anders. 

Schau man mal

## Slide 14 ##

Null Pointers sind ein Problem auch mit Coverity aber die Anzahl der Falschpostive waren niedriger.

## Slide 15 ##

Der Kontrollfluss ist eines der stärksten Features von Coverity. 
j
Kein Wunder, dass er dabei besser funktioniert als Helix.

## Slide 16 ##

Arithmetic Fehler und Dead Code sieht wie gut als Helix. Diesen Fehler kann fast jedes SCA Tool herausfinden.

## Slide 17 ##

## Das Coverity Résumé ##

* Datenfluss und Kontrollfluss Analyse sieht toll bei Coverity aus
* Die Analyse dauert 30 Minuten im Vergleich zur 8 Stunden von Helix.
* Ein paar Dateien waren herausgefiltert.

## Zusammenfassung ##

Ich stelle mir die Frage: Was haben sie gemeinsam?

Die Antwort ist leider: fast nix.

### Slide 19, 20 ###

Die Kategorien der Fehler sind anders sogar mit der Probleme Fehlerfiltern.

### Slide 21, 22 ##

Von der Ergebnisse wurden nur 2 Dateien mit dem gleichen Fehlertyp in beiden Tools gefunden.

### Slide 23: Unsere Empfehlung wegen des Ergebnis ###

Beide Tools präsentieren komplementäre Ergebnisse! 
Daher ist es sinnvoll, beide in einem Projekt anzuwenden.

Wenn wir nur einen auswählen würden, würden wir wahrscheinlich Coverity für die Datenflussanalyse wählen.
